{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is working okok"
      ],
      "metadata": {
        "id": "wv7dASC2RiN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Hackron/NewDatasetForTraining.xlsx\")\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "categorical_features = ['Item names', 'Item identifier', 'Item type', 'Item fat content',\n",
        "                        'Customer type', 'Seasonlity', 'Weather condition', 'Festive Season','Purchase frequency','Consumption time','Discounts and promotions','Time of day','Day of the week']\n",
        "numerical_features = ['Item weight', 'Sales', 'Stock availablity',\n",
        "                      'Competitor price','Demand surge', 'Supplier cost', 'Delivery cost', 'Warehouse stock',\n",
        "                        'Ratings']\n",
        "\n",
        "# Fill missing values\n",
        "for col in categorical_features:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "for col in numerical_features:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Store encoders for inverse transform if needed\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "df['Days to Expiry'] = (pd.to_datetime(df['Expiry Date']) - pd.to_datetime(df['Current Date'])).dt.days\n",
        "df.drop(columns=['Current Date', 'Expiry Date'], inplace=True)\n",
        "\n",
        "# Step 4: Define Features and Target Variable\n",
        "X = df.drop(columns=['Price'])\n",
        "y = df['Price']\n",
        "\n",
        "# Step 5: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Build the Neural Network Model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Step 8: Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "\n",
        "# Step 10: Save the Model\n",
        "model.save(\"dynamic_pricing_nn_model.h5\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mosADw0oJ0-c",
        "outputId": "a0f546b3-04b3-48c2-b944-765360d0c423"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 46235.3242 - mae: 125.5217 - val_loss: 1879.0251 - val_mae: 34.9973\n",
            "Epoch 2/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2096.0286 - mae: 34.9236 - val_loss: 952.2462 - val_mae: 25.3515\n",
            "Epoch 3/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1359.0543 - mae: 27.3937 - val_loss: 593.3502 - val_mae: 19.4122\n",
            "Epoch 4/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1082.8864 - mae: 22.7658 - val_loss: 341.8879 - val_mae: 13.5580\n",
            "Epoch 5/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 884.2598 - mae: 19.4975 - val_loss: 264.5878 - val_mae: 11.4995\n",
            "Epoch 6/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 824.0749 - mae: 18.5422 - val_loss: 221.0939 - val_mae: 10.2943\n",
            "Epoch 7/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 818.3757 - mae: 17.7453 - val_loss: 282.9474 - val_mae: 11.0985\n",
            "Epoch 8/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 783.5278 - mae: 17.0526 - val_loss: 197.3735 - val_mae: 9.1869\n",
            "Epoch 9/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 769.9744 - mae: 16.7607 - val_loss: 210.2924 - val_mae: 8.9763\n",
            "Epoch 10/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 647.2217 - mae: 15.2438 - val_loss: 150.8960 - val_mae: 8.1765\n",
            "Epoch 11/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 749.5220 - mae: 15.7749 - val_loss: 156.2413 - val_mae: 8.0526\n",
            "Epoch 12/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 698.5618 - mae: 15.7143 - val_loss: 157.5506 - val_mae: 8.3559\n",
            "Epoch 13/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 705.1307 - mae: 15.4303 - val_loss: 166.0757 - val_mae: 8.0272\n",
            "Epoch 14/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 648.1361 - mae: 14.9559 - val_loss: 151.4397 - val_mae: 8.1494\n",
            "Epoch 15/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 699.4357 - mae: 14.8166 - val_loss: 195.9166 - val_mae: 8.2287\n",
            "Epoch 16/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 718.3634 - mae: 15.3123 - val_loss: 241.5412 - val_mae: 9.1872\n",
            "Epoch 17/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 690.5825 - mae: 14.7937 - val_loss: 165.0769 - val_mae: 8.1202\n",
            "Epoch 18/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 696.6840 - mae: 14.8149 - val_loss: 207.7209 - val_mae: 8.5519\n",
            "Epoch 19/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 685.0831 - mae: 14.7944 - val_loss: 154.1898 - val_mae: 8.3408\n",
            "Epoch 20/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 657.2875 - mae: 14.1906 - val_loss: 138.9676 - val_mae: 7.5095\n",
            "Epoch 21/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 579.5958 - mae: 13.7371 - val_loss: 156.8493 - val_mae: 8.0956\n",
            "Epoch 22/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 747.5334 - mae: 14.7887 - val_loss: 250.9640 - val_mae: 8.7946\n",
            "Epoch 23/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 702.2653 - mae: 14.5153 - val_loss: 262.0741 - val_mae: 9.9437\n",
            "Epoch 24/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 631.7195 - mae: 13.9313 - val_loss: 139.9622 - val_mae: 7.4443\n",
            "Epoch 25/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 650.3538 - mae: 13.9419 - val_loss: 169.8680 - val_mae: 8.5388\n",
            "Epoch 26/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 702.6160 - mae: 14.1837 - val_loss: 162.6922 - val_mae: 7.5058\n",
            "Epoch 27/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 543.7448 - mae: 13.1729 - val_loss: 145.4654 - val_mae: 7.6991\n",
            "Epoch 28/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 570.1793 - mae: 13.3790 - val_loss: 152.4740 - val_mae: 7.5959\n",
            "Epoch 29/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 621.0204 - mae: 13.2723 - val_loss: 332.9707 - val_mae: 11.6335\n",
            "Epoch 30/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 645.1353 - mae: 13.8187 - val_loss: 132.7491 - val_mae: 7.1688\n",
            "Epoch 31/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 534.7136 - mae: 12.6090 - val_loss: 219.7965 - val_mae: 9.4342\n",
            "Epoch 32/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 601.2754 - mae: 13.4758 - val_loss: 145.5307 - val_mae: 7.6817\n",
            "Epoch 33/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 507.3995 - mae: 12.8153 - val_loss: 209.2051 - val_mae: 8.5880\n",
            "Epoch 34/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 630.8730 - mae: 13.4532 - val_loss: 241.4296 - val_mae: 8.8572\n",
            "Epoch 35/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 659.6860 - mae: 13.4842 - val_loss: 223.3551 - val_mae: 8.9953\n",
            "Epoch 36/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 717.2559 - mae: 13.9816 - val_loss: 148.5801 - val_mae: 7.9327\n",
            "Epoch 37/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 654.4432 - mae: 13.5899 - val_loss: 139.2148 - val_mae: 7.6320\n",
            "Epoch 38/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 724.3921 - mae: 13.8803 - val_loss: 143.3722 - val_mae: 7.3445\n",
            "Epoch 39/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 658.6550 - mae: 13.2854 - val_loss: 145.8922 - val_mae: 7.3740\n",
            "Epoch 40/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 669.7724 - mae: 13.5040 - val_loss: 139.6110 - val_mae: 7.2918\n",
            "Epoch 41/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 634.0731 - mae: 13.0641 - val_loss: 155.0219 - val_mae: 7.3143\n",
            "Epoch 42/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 468.2319 - mae: 12.4006 - val_loss: 132.3782 - val_mae: 6.8193\n",
            "Epoch 43/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 657.3305 - mae: 13.1565 - val_loss: 134.0413 - val_mae: 7.2979\n",
            "Epoch 44/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 614.7136 - mae: 13.4205 - val_loss: 145.2697 - val_mae: 7.3742\n",
            "Epoch 45/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 602.8074 - mae: 13.0523 - val_loss: 153.2852 - val_mae: 7.6189\n",
            "Epoch 46/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 558.2758 - mae: 12.5762 - val_loss: 140.5460 - val_mae: 7.2774\n",
            "Epoch 47/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 629.3573 - mae: 13.2247 - val_loss: 166.2767 - val_mae: 7.8669\n",
            "Epoch 48/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 509.8637 - mae: 12.4197 - val_loss: 175.9614 - val_mae: 8.2903\n",
            "Epoch 49/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 554.4094 - mae: 12.8757 - val_loss: 276.1833 - val_mae: 8.8482\n",
            "Epoch 50/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 553.6312 - mae: 12.5880 - val_loss: 135.7957 - val_mae: 7.1757\n",
            "Epoch 51/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 574.2404 - mae: 12.6755 - val_loss: 133.9532 - val_mae: 7.1004\n",
            "Epoch 52/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 621.8329 - mae: 13.0650 - val_loss: 134.4608 - val_mae: 7.1242\n",
            "Epoch 53/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 536.1615 - mae: 12.4686 - val_loss: 155.0690 - val_mae: 7.0288\n",
            "Epoch 54/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 546.3997 - mae: 12.6420 - val_loss: 688.5385 - val_mae: 15.1761\n",
            "Epoch 55/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 664.1792 - mae: 13.4307 - val_loss: 174.8926 - val_mae: 7.7415\n",
            "Epoch 56/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 507.8700 - mae: 12.3702 - val_loss: 242.5901 - val_mae: 9.1562\n",
            "Epoch 57/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 580.7546 - mae: 12.5705 - val_loss: 143.0150 - val_mae: 7.7974\n",
            "Epoch 58/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 616.8478 - mae: 12.7697 - val_loss: 180.8950 - val_mae: 7.3432\n",
            "Epoch 59/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 777.6801 - mae: 13.9528 - val_loss: 140.5612 - val_mae: 7.1400\n",
            "Epoch 60/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 600.6738 - mae: 12.6816 - val_loss: 178.2135 - val_mae: 7.5662\n",
            "Epoch 61/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 557.5333 - mae: 12.5257 - val_loss: 135.8820 - val_mae: 7.4237\n",
            "Epoch 62/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 592.0020 - mae: 12.5570 - val_loss: 123.7222 - val_mae: 6.8107\n",
            "Epoch 63/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 566.0095 - mae: 12.5395 - val_loss: 130.3824 - val_mae: 7.0125\n",
            "Epoch 64/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 494.7589 - mae: 11.9808 - val_loss: 128.5692 - val_mae: 6.9154\n",
            "Epoch 65/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 518.4142 - mae: 12.3798 - val_loss: 204.4774 - val_mae: 8.3283\n",
            "Epoch 66/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 522.5611 - mae: 12.3748 - val_loss: 149.7430 - val_mae: 7.2255\n",
            "Epoch 67/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 609.0284 - mae: 12.4504 - val_loss: 230.7420 - val_mae: 8.5682\n",
            "Epoch 68/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 594.7761 - mae: 12.7323 - val_loss: 134.6263 - val_mae: 7.4505\n",
            "Epoch 69/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 600.5231 - mae: 12.7744 - val_loss: 311.3655 - val_mae: 10.0505\n",
            "Epoch 70/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 603.5175 - mae: 12.9815 - val_loss: 194.2387 - val_mae: 8.2541\n",
            "Epoch 71/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 561.3442 - mae: 12.5348 - val_loss: 180.4273 - val_mae: 7.8041\n",
            "Epoch 72/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 456.4409 - mae: 11.5716 - val_loss: 136.9564 - val_mae: 7.1832\n",
            "Epoch 73/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 523.0151 - mae: 12.1828 - val_loss: 309.9081 - val_mae: 10.4124\n",
            "Epoch 74/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 628.9583 - mae: 13.0215 - val_loss: 163.5024 - val_mae: 7.3757\n",
            "Epoch 75/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 595.6857 - mae: 12.6451 - val_loss: 130.4359 - val_mae: 6.8245\n",
            "Epoch 76/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 533.3674 - mae: 12.2280 - val_loss: 212.6575 - val_mae: 7.9015\n",
            "Epoch 77/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 551.9669 - mae: 12.5047 - val_loss: 151.8172 - val_mae: 7.4146\n",
            "Epoch 78/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 543.1656 - mae: 12.1798 - val_loss: 151.4806 - val_mae: 7.4515\n",
            "Epoch 79/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 625.7543 - mae: 12.9283 - val_loss: 186.3958 - val_mae: 7.7248\n",
            "Epoch 80/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 483.2236 - mae: 12.0060 - val_loss: 148.5964 - val_mae: 7.4915\n",
            "Epoch 81/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 467.4553 - mae: 11.8053 - val_loss: 150.7959 - val_mae: 7.8316\n",
            "Epoch 82/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 630.1739 - mae: 12.5956 - val_loss: 248.2753 - val_mae: 8.8599\n",
            "Epoch 83/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 613.1141 - mae: 12.8540 - val_loss: 251.3048 - val_mae: 9.0240\n",
            "Epoch 84/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 573.4530 - mae: 12.4313 - val_loss: 182.8293 - val_mae: 7.5969\n",
            "Epoch 85/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 590.9059 - mae: 12.5654 - val_loss: 177.6798 - val_mae: 7.5487\n",
            "Epoch 86/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 575.7123 - mae: 12.4115 - val_loss: 145.0005 - val_mae: 7.5582\n",
            "Epoch 87/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 492.1800 - mae: 11.9820 - val_loss: 174.9110 - val_mae: 7.4201\n",
            "Epoch 88/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 590.3423 - mae: 12.5998 - val_loss: 228.3376 - val_mae: 8.6312\n",
            "Epoch 89/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 611.5240 - mae: 12.8248 - val_loss: 130.6948 - val_mae: 7.0526\n",
            "Epoch 90/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 567.7487 - mae: 12.4030 - val_loss: 167.1111 - val_mae: 8.2425\n",
            "Epoch 91/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 581.3578 - mae: 12.1902 - val_loss: 142.2580 - val_mae: 7.2495\n",
            "Epoch 92/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 583.1384 - mae: 12.4226 - val_loss: 419.4032 - val_mae: 10.2589\n",
            "Epoch 93/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 624.9930 - mae: 12.2816 - val_loss: 133.8202 - val_mae: 7.1991\n",
            "Epoch 94/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 604.9927 - mae: 12.2876 - val_loss: 132.6418 - val_mae: 7.1813\n",
            "Epoch 95/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 592.3363 - mae: 12.1324 - val_loss: 207.1657 - val_mae: 7.9232\n",
            "Epoch 96/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 531.8245 - mae: 11.9776 - val_loss: 128.2864 - val_mae: 6.6150\n",
            "Epoch 97/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 576.6246 - mae: 12.2595 - val_loss: 216.1036 - val_mae: 8.4725\n",
            "Epoch 98/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 567.1269 - mae: 12.3832 - val_loss: 271.3902 - val_mae: 8.9737\n",
            "Epoch 99/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 496.8976 - mae: 11.9897 - val_loss: 175.5782 - val_mae: 8.2616\n",
            "Epoch 100/100\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 581.7040 - mae: 12.2873 - val_loss: 134.3904 - val_mae: 7.2339\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 7.2338650843452\n",
            "RMSE: 11.592683537052332\n",
            "R² Score: 0.9965310801714515\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_price(sample_input):\n",
        "    input_data = {}\n",
        "\n",
        "    # Encode categorical features\n",
        "    for col in categorical_features:\n",
        "        if col in sample_input and sample_input[col] in label_encoders[col].classes_:\n",
        "            input_data[col] = label_encoders[col].transform([sample_input[col]])[0]\n",
        "        else:\n",
        "            input_data[col] = 0  # Default value if unseen category\n",
        "\n",
        "    # Convert numerical features\n",
        "    for col in numerical_features + ['Days to Expiry']:\n",
        "        if col in sample_input:\n",
        "            value = sample_input[col]\n",
        "            if isinstance(value, str) and '%' in value:\n",
        "                value = float(value.strip('%')) / 100  # Convert percentage to decimal\n",
        "            elif isinstance(value, str) and ':' in value:\n",
        "                hours, minutes = map(int, value.split(':'))\n",
        "                value = hours + minutes / 60  # Convert time to float\n",
        "            elif isinstance(value, str):\n",
        "                try:\n",
        "                    value = float(value)\n",
        "                except ValueError:\n",
        "                    value = 0  # Default for unexpected string values\n",
        "            input_data[col] = value\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    # Ensure column order matches training data\n",
        "    input_df = input_df[X.columns]\n",
        "\n",
        "    # Scale the data\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "\n",
        "    # Predict price\n",
        "    predicted_price = model.predict(input_scaled)[0][0]\n",
        "    return predicted_price\n",
        "\n",
        "# Sample Input\n",
        "sample_input = {\n",
        "    \"Item names\": \"Apple\",\n",
        "    \"Item identifier\": \"ITEM0057\",\n",
        "    \"Item type\": \"Fruits and Vegetables\",\n",
        "    \"Item fat content\": \"Low Fat\",\n",
        "    \"Customer type\": \"Urban\",\n",
        "    \"Seasonlity\": \"Medium\",\n",
        "    \"Weather condition\": \"Cold\",\n",
        "    \"Festive Season\": \"Diwali\",\n",
        "    \"Item weight\": 1.5,\n",
        "    \"Sales\": 150,\n",
        "    \"Purchase frequency\": 10,  # Converted from 'Daily'\n",
        "    \"Stock availablity\": 500,\n",
        "    \"Consumption time\": 8,  # Converted from 'Morning'\n",
        "    \"Competitor price\": 180,\n",
        "    \"Discounts and promotions\": \"26%\",  # Percentage conversion\n",
        "    \"Demand surge\": 1.67,\n",
        "    \"Supplier cost\": 133.9,\n",
        "    \"Delivery cost\": 15,\n",
        "    \"Warehouse stock\": 490,\n",
        "    \"Time of day\": \"10:00\",  # Needs conversion\n",
        "    \"Day of the week\": \"Sunday\",  # Needs encoding\n",
        "    \"Ratings\": 4,\n",
        "    \"Days to Expiry\": 31\n",
        "}\n",
        "\n",
        "# Predict\n",
        "predicted_price = predict_price(sample_input)\n",
        "print(f\"Predicted Price: {predicted_price}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erxsGqHILuUK",
        "outputId": "0c6899b1-1a86-4a32-af9a-129d1ed71785"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "Predicted Price: 191.40089416503906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This i am trying with more dense layers"
      ],
      "metadata": {
        "id": "vZs1IyOfRpfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Hackron/NewDatasetForTraining.xlsx\")\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "categorical_features = ['Item names', 'Item identifier', 'Item type', 'Item fat content',\n",
        "                        'Customer type', 'Seasonlity', 'Weather condition', 'Festive Season','Purchase frequency','Consumption time','Discounts and promotions','Time of day','Day of the week']\n",
        "numerical_features = ['Item weight', 'Sales', 'Stock availablity',\n",
        "                      'Competitor price','Demand surge', 'Supplier cost', 'Delivery cost', 'Warehouse stock',\n",
        "                        'Ratings']\n",
        "\n",
        "# Fill missing values\n",
        "for col in categorical_features:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "for col in numerical_features:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Store encoders for inverse transform if needed\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "df['Days to Expiry'] = (pd.to_datetime(df['Expiry Date']) - pd.to_datetime(df['Current Date'])).dt.days\n",
        "df.drop(columns=['Current Date', 'Expiry Date'], inplace=True)\n",
        "\n",
        "# Step 4: Define Features and Target Variable\n",
        "X = df.drop(columns=['Price'])\n",
        "y = df['Price']\n",
        "\n",
        "# Step 5: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Build the Optimized Neural Network Model\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Step 8: Train the Model with Early Stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "\n",
        "# Step 10: Save the Model\n",
        "model.save(\"dynamic_pricing_nn_model1.h5\")\n",
        "joblib.dump(scaler, \"scaler1.pkl\")\n",
        "joblib.dump(label_encoders, \"label_encoders1.pkl\")\n",
        "\n",
        "# Step 11: Test with Sample Input\n",
        "def predict_price(sample_input):\n",
        "    # Convert categorical values using stored LabelEncoders\n",
        "    input_data = {}\n",
        "    for col in categorical_features:\n",
        "        if col in sample_input:\n",
        "            input_data[col] = label_encoders[col].transform([sample_input[col]])[0]\n",
        "\n",
        "    # Add numerical features\n",
        "    for col in numerical_features + ['Days to Expiry']:\n",
        "        input_data[col] = sample_input[col]\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "\n",
        "    # Ensure column order matches training data\n",
        "    input_df = input_df[X.columns]\n",
        "\n",
        "    # Scale numerical values\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "\n",
        "    # Predict price\n",
        "    predicted_price = model.predict(input_scaled)[0][0]\n",
        "    return predicted_price\n",
        "\n",
        "# Example input\n",
        "sample_input = {\n",
        "    \"Item names\": \"Apple\",\n",
        "    \"Item identifier\": \"ITEM0057\",\n",
        "    \"Item type\": \"Fruits and Vegetables\",\n",
        "    \"Item fat content\": \"Low Fat\",\n",
        "    \"Customer type\": \"Urban\",\n",
        "    \"Seasonlity\": \"Medium\",\n",
        "    \"Weather condition\": \"Cold\",\n",
        "    \"Festive Season\": \"Diwali\",\n",
        "    \"Item weight\": 1.5,\n",
        "    \"Sales\": 150,\n",
        "    \"Purchase frequency\": \"Daily\",  # Converted from 'Daily'\n",
        "    \"Stock availablity\": 500,\n",
        "    \"Consumption time\": \"Morning\",  # Converted from 'Morning'\n",
        "    \"Competitor price\": 180,\n",
        "    \"Discounts and promotions\": \"26%\",  # Percentage conversion\n",
        "    \"Demand surge\": 1.67,\n",
        "    \"Supplier cost\": 133.9,\n",
        "    \"Delivery cost\": 15,\n",
        "    \"Warehouse stock\": 490,\n",
        "    \"Time of day\": \"10:00\",  # Needs conversion\n",
        "    \"Day of the week\": \"Sunday\",  # Needs encoding\n",
        "    \"Ratings\": 4,\n",
        "    \"Days to Expiry\": 31\n",
        "}\n",
        "\n",
        "predicted_price = predict_price(sample_input)\n",
        "print(f\"Predicted Price: {predicted_price}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Wr4wNFHRs5G",
        "outputId": "c856e797-96df-4bc9-ff4d-57488ab9255e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 42255.1406 - mae: 121.3666 - val_loss: 4995.5020 - val_mae: 41.6075\n",
            "Epoch 2/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 3121.3379 - mae: 38.0503 - val_loss: 821.2012 - val_mae: 20.4116\n",
            "Epoch 3/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2334.7703 - mae: 31.2501 - val_loss: 1233.1832 - val_mae: 19.1384\n",
            "Epoch 4/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2400.0315 - mae: 30.6981 - val_loss: 1647.8763 - val_mae: 19.3396\n",
            "Epoch 5/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2218.1948 - mae: 29.3169 - val_loss: 1074.1049 - val_mae: 17.0355\n",
            "Epoch 6/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1760.4718 - mae: 26.6005 - val_loss: 693.4694 - val_mae: 16.6974\n",
            "Epoch 7/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1582.4132 - mae: 25.2400 - val_loss: 823.7384 - val_mae: 15.8073\n",
            "Epoch 8/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1698.6548 - mae: 25.3160 - val_loss: 832.2961 - val_mae: 15.5144\n",
            "Epoch 9/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1545.6219 - mae: 24.5744 - val_loss: 1181.2384 - val_mae: 16.5009\n",
            "Epoch 10/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1489.1316 - mae: 23.7281 - val_loss: 486.9575 - val_mae: 13.6429\n",
            "Epoch 11/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1793.4865 - mae: 24.9270 - val_loss: 695.0097 - val_mae: 14.3149\n",
            "Epoch 12/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1615.7946 - mae: 23.9499 - val_loss: 1095.5968 - val_mae: 15.7110\n",
            "Epoch 13/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1254.7299 - mae: 22.1789 - val_loss: 836.4907 - val_mae: 14.8943\n",
            "Epoch 14/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1158.8781 - mae: 21.7335 - val_loss: 784.7292 - val_mae: 13.3922\n",
            "Epoch 15/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1191.5385 - mae: 21.2811 - val_loss: 294.6884 - val_mae: 12.0287\n",
            "Epoch 16/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1193.8964 - mae: 21.1024 - val_loss: 531.3376 - val_mae: 12.9258\n",
            "Epoch 17/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1078.3969 - mae: 20.7561 - val_loss: 401.6601 - val_mae: 13.0267\n",
            "Epoch 18/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1249.6598 - mae: 21.6642 - val_loss: 398.0566 - val_mae: 12.8244\n",
            "Epoch 19/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1078.4299 - mae: 20.6200 - val_loss: 516.5910 - val_mae: 13.3126\n",
            "Epoch 20/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1061.9971 - mae: 20.7181 - val_loss: 395.2432 - val_mae: 12.9761\n",
            "Epoch 21/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1094.7467 - mae: 20.8383 - val_loss: 320.1259 - val_mae: 11.4656\n",
            "Epoch 22/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1166.3059 - mae: 20.6385 - val_loss: 370.1115 - val_mae: 11.5685\n",
            "Epoch 23/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1133.6442 - mae: 21.1271 - val_loss: 392.9774 - val_mae: 13.3943\n",
            "Epoch 24/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 965.4579 - mae: 19.2460 - val_loss: 513.7469 - val_mae: 12.8962\n",
            "Epoch 25/200\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 950.8251 - mae: 19.1845 - val_loss: 426.7646 - val_mae: 12.5857\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 12.028664179431692\n",
            "RMSE: 17.16648875155216\n",
            "R² Score: 0.992393423304892\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y contains previously unseen labels: '10:00'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '10:00'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c33bbb23ff4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m }\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mpredicted_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted Price: {predicted_price}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c33bbb23ff4e>\u001b[0m in \u001b[0;36mpredict_price\u001b[0;34m(sample_input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Add numerical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: '10:00'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Hackron/NewDatasetForTraining.xlsx\")\n",
        "\n",
        "# Define categorical and numerical features\n",
        "categorical_features = ['Item names', 'Item identifier', 'Item type', 'Item fat content',\n",
        "                        'Customer type', 'Seasonlity', 'Weather condition', 'Festive Season',\n",
        "                        'Purchase frequency', 'Consumption time', 'Discounts and promotions',\n",
        "                        'Day of the week']\n",
        "\n",
        "numerical_features = ['Item weight', 'Sales', 'Stock availablity', 'Competitor price',\n",
        "                      'Demand surge', 'Supplier cost', 'Delivery cost', 'Warehouse stock',\n",
        "                      'Ratings']\n",
        "\n",
        "# Fill missing values\n",
        "for col in categorical_features:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "for col in numerical_features:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Convert Dates to Days to Expiry\n",
        "df['Days to Expiry'] = (pd.to_datetime(df['Expiry Date']) - pd.to_datetime(df['Current Date'])).dt.days\n",
        "df.drop(columns=['Current Date', 'Expiry Date'], inplace=True)\n",
        "\n",
        "# Convert 'Time of day' into sin & cos features\n",
        "df['Hour'] = pd.to_datetime(df['Time of day'], format='%H:%M').dt.hour\n",
        "df['Time_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
        "df['Time_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
        "df.drop(columns=['Time of day', 'Hour'], inplace=True)\n",
        "\n",
        "# Define Features and Target Variable\n",
        "X = df.drop(columns=['Price'])\n",
        "y = df['Price']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Optimized Neural Network Model\n",
        "model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model with optimized learning rate\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the Model with Early Stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=300, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "\n",
        "# Save the Model & Encoders\n",
        "model.save(\"final_dynamic_pricing_nn_model.h5\")\n",
        "joblib.dump(scaler, \"final_scaler.pkl\")\n",
        "joblib.dump(label_encoders, \"final_label_encoders.pkl\")\n",
        "\n",
        "# Function to Predict Price for New Input\n",
        "def predict_price(sample_input):\n",
        "    input_data = {}\n",
        "\n",
        "    # Encode categorical features\n",
        "    for col in categorical_features:\n",
        "        if col in sample_input:\n",
        "            if sample_input[col] in label_encoders[col].classes_:\n",
        "                input_data[col] = label_encoders[col].transform([sample_input[col]])[0]\n",
        "            else:\n",
        "                input_data[col] = 0  # Default if unseen category\n",
        "\n",
        "    # Convert numerical features\n",
        "    for col in numerical_features + ['Days to Expiry']:\n",
        "        if col in sample_input:\n",
        "            value = sample_input[col]\n",
        "            if isinstance(value, str) and '%' in value:\n",
        "                value = float(value.strip('%')) / 100  # Convert percentage to decimal\n",
        "            elif isinstance(value, str):\n",
        "                try:\n",
        "                    value = float(value)\n",
        "                except ValueError:\n",
        "                    value = 0  # Default for unexpected string values\n",
        "            input_data[col] = value\n",
        "\n",
        "    # Convert 'Time of day' into sin & cos\n",
        "    if 'Time of day' in sample_input:\n",
        "        hours, minutes = map(int, sample_input['Time of day'].split(':'))\n",
        "        input_data[\"Time_sin\"] = np.sin(2 * np.pi * hours / 24)\n",
        "        input_data[\"Time_cos\"] = np.cos(2 * np.pi * hours / 24)\n",
        "\n",
        "    # Convert to DataFrame and ensure column order\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "    input_df = input_df[X.columns]\n",
        "\n",
        "    # Scale the data\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "\n",
        "    # Predict price\n",
        "    predicted_price = model.predict(input_scaled)[0][0]\n",
        "    return predicted_price\n",
        "\n",
        "# Example Input\n",
        "sample_input = {\n",
        "    \"Item names\": \"Tomato\",\n",
        "    \"Item identifier\": \"ITEM0057\",\n",
        "    \"Item type\": \"Fruits and Vegetables\",\n",
        "    \"Item fat content\": \"Low Fat\",\n",
        "    \"Customer type\": \"Urban\",\n",
        "    \"Seasonlity\": \"Medium\",\n",
        "    \"Weather condition\": \"Cold\",\n",
        "    \"Festive Season\": \"Diwali\",\n",
        "    \"Item weight\": 1.5,\n",
        "    \"Sales\": 800,\n",
        "    \"Purchase frequency\": \"Daily\",\n",
        "    \"Stock availablity\": 50,\n",
        "    \"Consumption time\": \"Morning\",\n",
        "    \"Competitor price\": 60,\n",
        "    \"Discounts and promotions\": \"20%\",\n",
        "    \"Demand surge\": 1.67,\n",
        "    \"Supplier cost\": 38,\n",
        "    \"Delivery cost\": 10,\n",
        "    \"Warehouse stock\": 350,\n",
        "    \"Time of day\": \"10:00\",\n",
        "    \"Day of the week\": \"Sunday\",\n",
        "    \"Ratings\": 4,\n",
        "    \"Days to Expiry\": 23\n",
        "}\n",
        "\n",
        "predicted_price = predict_price(sample_input)\n",
        "print(f\"Predicted Price: {predicted_price}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO3VBTF2Ws4H",
        "outputId": "3adebcaf-40fb-41b4-f144-b6bfc588aed9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 52308.6992 - mae: 140.8433 - val_loss: 22841.6816 - val_mae: 95.4489\n",
            "Epoch 2/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4397.1484 - mae: 43.0921 - val_loss: 12413.6494 - val_mae: 49.2171\n",
            "Epoch 3/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2586.6516 - mae: 31.2353 - val_loss: 6230.8833 - val_mae: 43.8650\n",
            "Epoch 4/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2441.9006 - mae: 29.8316 - val_loss: 1499.3256 - val_mae: 24.8660\n",
            "Epoch 5/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1743.7781 - mae: 26.8097 - val_loss: 405.4280 - val_mae: 14.4055\n",
            "Epoch 6/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1900.8013 - mae: 27.4667 - val_loss: 543.4158 - val_mae: 14.7490\n",
            "Epoch 7/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1477.9669 - mae: 24.4874 - val_loss: 510.4757 - val_mae: 14.0566\n",
            "Epoch 8/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1718.6238 - mae: 25.8467 - val_loss: 577.0232 - val_mae: 14.7242\n",
            "Epoch 9/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1571.0254 - mae: 24.6608 - val_loss: 709.8096 - val_mae: 14.3954\n",
            "Epoch 10/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1390.3577 - mae: 23.5479 - val_loss: 424.4726 - val_mae: 12.5636\n",
            "Epoch 11/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1790.5605 - mae: 24.8361 - val_loss: 389.5316 - val_mae: 13.5827\n",
            "Epoch 12/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1468.4266 - mae: 23.5850 - val_loss: 391.5898 - val_mae: 12.9778\n",
            "Epoch 13/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1417.6083 - mae: 23.2726 - val_loss: 1112.5758 - val_mae: 16.1279\n",
            "Epoch 14/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1457.5339 - mae: 22.9473 - val_loss: 375.7046 - val_mae: 12.9212\n",
            "Epoch 15/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1130.3993 - mae: 21.4416 - val_loss: 386.0933 - val_mae: 12.6132\n",
            "Epoch 16/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1411.6591 - mae: 22.3399 - val_loss: 315.5025 - val_mae: 12.3579\n",
            "Epoch 17/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1205.0479 - mae: 21.3750 - val_loss: 751.9060 - val_mae: 14.4632\n",
            "Epoch 18/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1307.7140 - mae: 22.1053 - val_loss: 408.6932 - val_mae: 13.3487\n",
            "Epoch 19/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1383.3374 - mae: 22.4320 - val_loss: 319.9131 - val_mae: 12.7235\n",
            "Epoch 20/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1271.4739 - mae: 21.0476 - val_loss: 957.6001 - val_mae: 14.7239\n",
            "Epoch 21/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1128.0221 - mae: 20.9846 - val_loss: 366.9726 - val_mae: 13.6950\n",
            "Epoch 22/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1155.7224 - mae: 20.5271 - val_loss: 562.7360 - val_mae: 12.9477\n",
            "Epoch 23/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1096.8546 - mae: 20.7088 - val_loss: 567.3320 - val_mae: 13.3775\n",
            "Epoch 24/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1027.9469 - mae: 20.7861 - val_loss: 291.0088 - val_mae: 11.3506\n",
            "Epoch 25/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1016.3729 - mae: 20.1554 - val_loss: 587.2299 - val_mae: 13.1855\n",
            "Epoch 26/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1141.9714 - mae: 20.1616 - val_loss: 308.8896 - val_mae: 12.4067\n",
            "Epoch 27/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1118.0099 - mae: 20.3973 - val_loss: 351.8276 - val_mae: 12.6311\n",
            "Epoch 28/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1068.5811 - mae: 20.2704 - val_loss: 539.7021 - val_mae: 14.7482\n",
            "Epoch 29/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1292.3359 - mae: 21.1091 - val_loss: 291.9575 - val_mae: 11.9100\n",
            "Epoch 30/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1073.6471 - mae: 19.5932 - val_loss: 336.1376 - val_mae: 11.1734\n",
            "Epoch 31/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 884.0066 - mae: 18.8972 - val_loss: 404.3611 - val_mae: 12.0759\n",
            "Epoch 32/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 938.7568 - mae: 19.2077 - val_loss: 416.2097 - val_mae: 13.1515\n",
            "Epoch 33/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 971.0461 - mae: 19.1531 - val_loss: 571.4782 - val_mae: 12.8380\n",
            "Epoch 34/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 890.2627 - mae: 18.9003 - val_loss: 258.3101 - val_mae: 10.7853\n",
            "Epoch 35/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1005.6752 - mae: 18.7841 - val_loss: 321.8796 - val_mae: 12.4302\n",
            "Epoch 36/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 910.8597 - mae: 18.9240 - val_loss: 425.0871 - val_mae: 12.5932\n",
            "Epoch 37/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 956.0238 - mae: 19.0496 - val_loss: 347.9695 - val_mae: 13.2747\n",
            "Epoch 38/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 916.0433 - mae: 18.6267 - val_loss: 315.2481 - val_mae: 11.4877\n",
            "Epoch 39/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1034.8805 - mae: 19.6810 - val_loss: 571.4101 - val_mae: 12.9237\n",
            "Epoch 40/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 922.3874 - mae: 18.9661 - val_loss: 799.7407 - val_mae: 13.0678\n",
            "Epoch 41/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1034.1047 - mae: 19.2004 - val_loss: 410.8639 - val_mae: 11.8983\n",
            "Epoch 42/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 878.3910 - mae: 18.2897 - val_loss: 411.4491 - val_mae: 12.3813\n",
            "Epoch 43/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 955.8612 - mae: 19.0542 - val_loss: 363.5045 - val_mae: 11.7196\n",
            "Epoch 44/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 876.0655 - mae: 18.2133 - val_loss: 407.6369 - val_mae: 11.1299\n",
            "Epoch 45/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 974.2391 - mae: 18.5795 - val_loss: 440.2773 - val_mae: 13.0470\n",
            "Epoch 46/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 889.9949 - mae: 18.0157 - val_loss: 309.3108 - val_mae: 11.9403\n",
            "Epoch 47/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 840.0723 - mae: 18.0988 - val_loss: 275.1358 - val_mae: 10.9737\n",
            "Epoch 48/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 778.2703 - mae: 17.3640 - val_loss: 518.5904 - val_mae: 13.5688\n",
            "Epoch 49/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 975.9135 - mae: 18.3148 - val_loss: 405.0952 - val_mae: 12.6508\n",
            "Epoch 50/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 986.2461 - mae: 18.0237 - val_loss: 357.9171 - val_mae: 12.2409\n",
            "Epoch 51/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 849.0555 - mae: 18.2859 - val_loss: 316.9487 - val_mae: 11.7549\n",
            "Epoch 52/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 799.9377 - mae: 17.1783 - val_loss: 334.3436 - val_mae: 10.1703\n",
            "Epoch 53/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 869.4219 - mae: 17.4269 - val_loss: 299.7974 - val_mae: 11.4444\n",
            "Epoch 54/300\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 859.6464 - mae: 17.5654 - val_loss: 625.7346 - val_mae: 13.9225\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 10.785310528923482\n",
            "RMSE: 16.07202925741264\n",
            "R² Score: 0.9933324277470581\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "Predicted Price: 52.778724670410156\n"
          ]
        }
      ]
    }
  ]
}